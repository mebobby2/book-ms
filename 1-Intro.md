# The DevOps Ideal
CI/CD means more than Jenkins and writing scripts. It relates to almost every aspect of software development. CI/CD cannot be done without making architectural decisions. Similar can be said for tests, configurations, environments, fail-over, and so on. To create a successful implementation of CI/CD, we need to make a lot of changes that, on the first look, do not seem to be directly related. We need to apply some patterns and practices from the very beginning. We have to think about architecture, testing, coupling, packaging, fault tolerance, and many other things. CI/CD requires us to influence almost every aspect of software development.

To be truly proficient with CI/CD, we need to be much more than experts in operations. DevOps movement was a significant improvement that combined traditional operations with advantages that development could bring. But it's not enough. We need to know and influence architecture, testing, development, operations and even customer negotiations if we want to gain all the benefits that CI/CD can bring. We need to go back to the beginning and not only make sure that operations are automated but that the whole system is designed in the way that it can be automated, fast, scalable, fault-tolerant, with zero-downtime, easy to monitor, and so on. We cannot accomplish this by simply automating manual procedures and employing a single do-it-all tool. We need to go much deeper than that and start refactoring the whole system both on the technological as well as the procedural level.

## Continuous Integration, Delivery, and Deployment
eXtreme Programming (XP) bought forth the thinking of *continuous integration* (CI). You should not wait until the last moment to integrate! In the waterfall era, such a thing was not so obvious as today. We implemented a continuous integration pipeline and started checking out every commit, running static analysis, unit and functional tests, packaging, deploying and running integration tests.

Later on, *continuous delivery* (CD) started to take ground, and we would have confidence that every commit that passed the whole pipeline could be deployed to production. We could do even better and not only attest that each build is production ready, but apply *continuous deployment* and deploy every build without waiting for (manual) confirmation from anyone. And the best part of all that was that everything was fully automated.

## Architecture
Monolithic applications are hard to 'modernize' due to its tight coupling, age, and outdated technology, changing parts. Commonly taken option was to start building the new system and, in parallel, maintain the old one until everything is done. That was always a disaster. It can take years to finish such a project, and we all know what happens with things planned for such a long term. That’s not even the waterfall approach. That’s like standing at the bottom of Niagara Falls and wondering why you get wet.

Microservices allow us to build many small independent services that can be maintained by small teams, have codebase that can be understood in no time, being able to change framework, programming language or a database without affecting the rest of the system and being able to deploy it the individual services independently. We could, finally, start taking parts of the monolithic application out without putting the whole system at (significant) risk.

However, benefits came with downsides. Deploying and maintaining a vast number of services turned out to be a heavy burden. We had to compromise and start standardizing services (killing innovation), we created shared libraries (coupling again), we were deploying them in groups (slowing everything), and so on. In other words, we had to remove the benefits microservices were supposed to bring. And let’s not even speak of configurations and the mess they created inside servers. We had enough problems like that with monoliths. Microservices only multiplied them.

## Deployments
With enough time, any server maintained manually becomes full of “things”. Libraries, executables, configurations, gremlins and trolls. The only thing all the servers had in common was that they were all different, and no one could be sure that software tested in, let’s say, pre-production environment will behave the same when deployed to production.

Differences accumulate with time unless you have a repeatable and reliable automated process instead of manual human interventions. If such a thing would exist, we could create immutable servers. Instead of deploying applications to existing servers and go down the path of accumulating differences, we could create a new VM as part of the CI/CD pipeline. So, instead of creating JARs, WAR, DLLs, and so on, we were creating VMs. Every time there is a new release it would come as a complete server built from scratch. That way we would know that what was tested is what goes into production. Create new VM with software deployed, test it and switch your production router to point from the old to the new one. It was awesome, except that it was slow and resource demanding. Having a separate VM for each service is overkill.

## Orchestration
The orchestration was the key. Puppet and Chef proved to be a big help. Programming everything related to servers setup and deployment was a huge improvement. Not only that the time needed to setup servers and deploy software dropped drastically, but we could, finally, accomplish a more reliable process.

However, given enough time, Puppet and Chef scripts and configurations turn into an enormous pile of ****. Maintaining them tends to become a nightmare in itself.

## The Light at the End of the Deployment Pipeline
Many, if not all of the problems we had before are now solved. Ansible proved that orchestration does not need to be complicated to set up nor hard to maintain. With the appearance of Docker, containers are slowly replacing VMs as the preferable way to create immutable deployments. New operating systems are emerging and fully embracing containers as first class citizens. Tools for service discovery are showing us new horizons. Swarm, Kubernetes and Mesos/DCOS are opening doors into areas that were hard to imagine only a few years ago.

Microservices are slowly becoming the preferred way to build big, easy to maintain and highly scalable systems thanks to tools like Docker, CoreOS, etcd, Consul, Fleet, Mesos, Rocket, and others. The idea was always great, but we did not have the tools to make it work properly. Now we do!
